\section{Methods} \label{sec:methods}
\subsection{Simulated Data} \label{subsec:simdata}
We first used seed phylogenies generated via a birth death model (with birth and death rate equal to $5\times 10^{-3}$ and with a 5\% chance of being sampled) in the R package TreeSim \citep{TreeSim, Stradler13, Boskova14}, and assign a clock to these trees (with rate$=3\times 10^{-3}$ substitutions per site/day, and with noise pulled from a normal distribution with $\sigma=5\times 10^{-5}$.
We then used INDELible 1.03 \citep{Indelible09} to simulated nucleotide sequences along these seed phylogenies with an HKY85 \citep{HKY85} substitution model, and  parameters set from a maximum likelihood estimate of published within host data \citep{McCloskey14}. 
From these simulated sequences a maximum likelihood phylogeny was then reconstructed with RAxML \citep{Raxml14}. In total, we generated 50 trees without latency, each having 100 tips. 

To generate the trees that had observable latency, we took a similar approach to that of \cite{Immonen14}. 
We randomly selected a branch within a seed phylogeny then scaled it by a random value between $0.1$ and $0.9$, to assure that the phylogeny had sufficiently many latent tips, we rejected all such trees that had less than 40 and more than 60 latent tips. 
We then recorded both the simulated age and simulated collection time, then generated sequences along these latent seed trees with INDELible 1.03 \citep{Indelible09} and the same parameters above. 
From these simulated sequences a maximum likelihood phylogeny was then reconstructed with RAxML \citep{Raxml14}.
For these data, we generated an additional 50 trees with 100 tips, bringing the total amount of simulated data to 100 trees with 100 tips each. 


\subsection{Data Collection} \label{subsec:dcollection}
We collected two distinct sets of data for our experiments, the first is the {\em plasma} data-set, which contains data collected from studies in which only viral RNA had been collected. 
The second data-set, the {\em mixed} data-set, contained sequences from studies in which both viral RNA in plasma and integrated viral DNA from PBMC cells had been collected.

Specifically, the {\em plasma} data-set \citep{McCloskey14} is a previously assembled and cleaned data-set from the Los Alamos HIV Database (\href{http://www.hiv.lanl.gov/}{http://www.hiv.lanl.gov/}). 
This data set contains 335 data sets from 232 patients, with a total of over 19,000 sequences \citep{McCloskey14} containing mostly longitudinal samples collected from plasma. 
From this set we selected treatment naive patients with two or more clonal or single-genome sequences available at each time point, with a known time-line relative to one of several reference points: HIV infection, seroconversion or presentation of symptomatic seroconversion illness. 
The samples were collected within 6 months (186 days) of this reference point, and at least one of the subsequent (“follow-up”) time points occurred a minimum of 6 months after baseline.
Patients who had been classified as ``superinfected'' were also excluded, and the samples were taken from the {\em env} region of the HIV-1 genome \citep{McCloskey14}. 
From this data-set we use \anote{X} plasma samples from \anote{Y} specific patients. 


For the {\em mixed} data-set, we used the Los Almos advanced search interface (\href{http://www.hiv.lanl.gov/}{http://www.hiv.lanl.gov/}) to identify subjects who have had both viral RNA and integrated DNA from PBMC cells collected over various time points -- we selected individuals from published studies that had well-characterized partial {\em env}. 
There was no condition for the time between sample collections. 
\anote{X} individuals were included with plasma samples from at least 2 time points (table \ref{tab:patients}).
A total of \anote{X} partial {\em env} sequences from blood plasma, and \anote{Y} partial {\em env} sequences from PBMC cells were used in our analysis. 
We utilized samples from treatment naive individuals \citep{Shankarappa99, Novitsky09}, from individuals that had been treated via HAART \citep{Llewellyn06}, and individuals that had been on HAART, but ceased treatment \citep{Fischer04}. 
 
\begin{table*}[!ht]
\def\arraystretch{1.3}%
\begin{center}
\begin{tabular}{llrrrrrr} 

Reference & Patient ID & \multicolumn{3}{c}{Sequences} & \multicolumn{3}{c}{Time points}\\ 
 &  & Plasma & PBMC & Total & Plasma & PBMC & Total\\
\hline
\cite{Shankarappa99} & 820 &       50 &       87 &      137 &        5 &       10 &       15  \\
& 821 &      76 &      192 &      268 &        7 &       17 &       17   \\ 
& 822 &      32 &       98 &      130 &        3 &       10 &       10   \\ 
& 824 &     100 &      107 &      207 &        7 &        9 &       13   \\ 
& 10137 &     24 &      106 &      130 &        2 &       10 &       12  \\ 
& 10138 &     82 &      119 &      201 &        6 &       13 &       16  \\
& 10586 &     16 &      121 &      137 &        2 &       12 &       14  \\ 
& 13889 &    151 &      132 &      283 &       13 &       14 &       18  \\ 
 \cite{Fischer04} & 10769 &    229 &       96 &      325 &       10 &        4 &       11  \\ 
& 10770 &    190 &       31 &      221 &       11 &        2 &       11  \\ 
\cite{Llewellyn06} & 16616 &     16 &       95 &      111 &        2 &        5 &        5  \\ 
& 16617 &     16 &       89 &      105 &        2 &        5 &        5  \\ 
& 16618 &     25 &       75 &      100 &        2 &        5 &        6  \\ 
& 16619 &     13 &       60 &       73 &        2 &        6 &        6  \\ 
\cite{Novitsky09} & 34382 &     37 &        5 &       42 &        3 &        1 &        4  \\ 
& 34391 &     13 &       38 &       51 &        3 &        5 &        6  \\ 
& 34393 &     25 &       74 &       99 &        2 &        6 &        7  \\ 
& 34396 &      23 &       46 &       69 &        2 &        5 &        5 \\ 
& 34397 &      26 &       25 &       51 &        2 &        2 &        4  \\ 
& 34399 &      61 &       88 &      149 &        5 &        9 &       12  \\ 
& 34400 &      54 &       23 &       77 &        4 &        1 &        5  \\ 
& 34405 &      27 &       29 &       56 &        3 &        2 &        4  \\ 
& 34408 &       5 &       73 &       78 &        2 &        8 &        8 \\ 
& 34410 &      35 &       60 &       95 &        2 &        6 &        6 \\ 
& 34411 &      25 &       43 &       68 &        3 &        3 &        6   \\ \hline
\end{tabular}
\end{center}
  \caption{Summary of all the patient data collected from the HIV LANL database -- Patient ID corresponds to the Los Alamos database's Patient ID \citep{LosAlamos}.
   }\label{tab:patients} 
\end{table*}

\subsection{Sequence Aligment} \label{subsec:seqalign}
When applicable, we used the built in MUSCLE 3.8.31 \citep{Muscle04} interface within AliView \citep{AliView14} to align the sequences, which were then visually inspected and cleaned. 
Alignments were trimmed to the interval in which at least  50\% of sequences within the data set had sequence coverage over each codon.
Our simulated sequences contained no insertions or deletions, so no alignment was necessary. 
Additionally, the plasma data-set \citep{McCloskey14} had already been aligned and cleaned, so no further alignment was performed.


\subsection{Phylogenetic Reconstruction} \label{subsec:phylo}
In all experiments, our reconstructed phylogenies were maximum likelihood phylogenies reconstructed in RAxML \citep{Raxml14} using the GTR+$\Gamma$ model.
We did not attempt to use BEAST \citep{BEAST} to reconstruct any phylogenies from our data.
Leaving dates unspecified on the number of tips would have drastically increased the dimensionality of the problem, and we would therefore need a prohibitively large amount of samples from the MCMC chain to assure convergence to the posterior distribution.


To root these trees, either outgroup rooting against the HIV-1 B ancestor reconstruction, or a modified version of RTT regression \citep{APE}. 
Classically, RTT regression considers all tips and chooses the root that minimizes some error metric between the maximum likelihood estimated clock and the tip dates. 
For this application, we simply ignore the tips that are undetermined (either censored or otherwise unknown) in the optimization of the root. 
Our synthetic data was only rooted using this modified RTT scheme, for all real data, both outgroup rooting and root-to-tip regression were used to root the phylogeny. 


\subsection{Date Reconstruction} \label{subsec:daterecon}
Once the trees had been rooted, to reconstruct dates, a general linear model with the normal family was constructed. 
The expected amount of evolution was taken to be the response and the sampling dates of the data were taken as the input. 
These models were only trained over the uncensored/plasma data for the simulated and real data sets respectively.

\subsection{Hypothesis Testing} \label{subsec:hypot}
To screen for data are amenable to our analysis, we used the nested log ratio test \citep{Ho14}. 
Our null model was a linear model with zero slope over the expected amount of evolution over time. 
We rejected any sequence data-sets whose phylogeny couldn't reject the null model with threshold $\alpha=0.01$.


\subsection{Experiments} \label{subsec:experiments}
Our first validation experiment on synthetic data was preformed as such: First we choose 50\% of the tips of a phylogeny and censored those dates, we next used root-to-tip regression \citep{APE} on the remaining data to calibrate the molecular clock for the tree. Finally, for each censored date, we reconstructed the expected date using the expected number of substitutions.

In all of our experiments, before any analysis is done, all times are re-scaled to lie between $0$ and $1$. This is calculated as $$ \text{new time} = \frac{\text{time} - \text{min time}}{\text{max time} - \text{min time}}$$ this gives normalized time. Errors are then calculated from this number, which we call normalized error. On the left figure \ref{fig:results1} shows a linear regression over either simulation time or time from a reference point (for patient data). On the right side of figure \ref{fig:results1}, is the superposition of an error density estimate for each data set within a category. 
