\section{Methods} \label{sec:methods}
\subsection{Simulated Data} \label{subsec:simdata}
We first used seed phylogenies generated via a birth death model (with birth and death rate equal to $7\times 10^{-3}$ and with a 99\% chance of being sampled) in the R package TreeSim \citep{TreeSim, Stradler13, Boskova14}, and assign a clock to these trees (with rate$ = \ 3\times 10^{-4}$ substitutions per site/day, and with noise pulled from a normal distribution with $\sigma = \ 1\times 10^{-4}$.
We then used INDELible 1.03 \citep{Indelible09} to simulated nucleotide sequences along these seed phylogenies with an HKY85 \citep{HKY85} substitution model, and  parameters set from a maximum likelihood estimate of published within host data \citep{McCloskey14}. 
From these simulated sequences a maximum likelihood phylogeny was then reconstructed with FastTree2 \citep{FastTree10}.
We attempted phylogenetic reconstruction with RAxML \citep{Raxml14}, but FastTree2 proved to be much faster with little (if any) effect on the results. 
In total, we generated 50 trees without latency, each having 100 tips. 

There do exist approaches to simulate latent sequences \citep{Immonen14},yet our assumptions about latency are more simplified than those of \cite{Immonen14} so we take a slightly different approach.
We assume at most one latency period per sample, and that once a cell becomes latent it remains latent. 
Therefore, to simulate latency, we choose 50 tips uniformly at random, then shorten their expected number substitutions and their branch time (the time between the tip, and the tip's parent) by some scalar amount drawn from a beta distribution with parameters $\alpha=1, \beta=200$.
We then recorded both the simulated age and simulated collection time and generated sequences along these latent seed trees with INDELible 1.03 \citep{Indelible09} and the same parameters above. 
From these simulated sequences a maximum likelihood phylogeny was then reconstructed with FastTree2 \citep{FastTree10}.
For these data, we generated an additional 50 trees with 100 tips, bringing the total amount of simulated data to 100 trees with 100 tips each. 


\subsection{Data Collection} \label{subsec:dcollection}
We collected two distinct sets of data for our experiments, the first is the {\em plasma} data set, which contains data collected from studies in which only viral RNA had been collected from individuals. 
The second data set, the {\em mixed} data set, contains sequences from studies in which both viral RNA in plasma and integrated viral DNA from PBMC cells had been collected.

Specifically, the {\em plasma} data set \citep{McCloskey14} is a previously assembled and cleaned data set from the Los Alamos HIV Database (\href{http://www.hiv.lanl.gov/}{http://www.hiv.lanl.gov/}). 
This data set contains 335 data sets from 232 patients, with a total of over 19,000 sequences \citep{McCloskey14} containing mostly longitudinal samples collected from plasma. 
From this set, we selected treatment naive patients with two or more clonal or single-genome sequences available at each time point, with a known time-line relative to one of several reference points: HIV infection, seroconversion or presentation of symptomatic seroconversion illness. 
The samples were collected within 186 days of this reference point, and at least one of the subsequent ``follow-up'' time points occurred a minimum of 6 months after baseline.
Patients who had been classified as ``superinfected'' were also excluded, and the samples were taken from the {\em env} region of the HIV-1 genome \citep{McCloskey14}. 
From this data set we use 550 plasma samples from 11 specific patients. 


For the {\em mixed} data set, we used the Los Almos advanced search interface (\href{http://www.hiv.lanl.gov/}{http://www.hiv.lanl.gov/}) to identify subjects who have had both viral RNA and integrated DNA from PBMC cells collected over various time points -- we selected individuals from published studies that had well-characterized partial {\em env} sequences. 
There was no condition for the time between sample collections. 
25 individuals were included with plasma samples from at least 2 time points (table \ref{tab:patients}).
A total of 1249 partial {\em env} sequences from blood plasma, and 1914 partial {\em env} sequences from PBMC cells were used in our analysis. 
We utilized samples from treatment naive individuals \citep{Shankarappa99, Novitsky09}, from individuals that had been treated via HAART \citep{Llewellyn06}, and individuals that had been on HAART, but ceased treatment \citep{Fischer04}. 
 
\begin{table*}[!ht]
\def\arraystretch{1.3}%
\begin{center}
\begin{tabular}{llrrrrrr} 

Reference & Patient ID & \multicolumn{3}{c}{Sequences} & \multicolumn{3}{c}{Time points}\\ 
 &  & Plasma & PBMC & Total & Plasma & PBMC & Total\\
\hline
\cite{Shankarappa99} & 820 &       50 &       87 &      137 &        5 &       10 &       15  \\
& 821 &      76 &      192 &      268 &        7 &       17 &       17   \\ 
& 822 &      32 &       98 &      130 &        3 &       10 &       10   \\ 
& 824 &     100 &      107 &      207 &        7 &        9 &       13   \\ 
& 10137 &     24 &      106 &      130 &        2 &       10 &       12  \\ 
& 10138 &     82 &      119 &      201 &        6 &       13 &       16  \\
& 10586 &     16 &      121 &      137 &        2 &       12 &       14  \\ 
& 13889 &    151 &      132 &      283 &       13 &       14 &       18  \\ 
 \cite{Fischer04} & 10769 &    229 &       96 &      325 &       10 &        4 &       11  \\ 
& 10770 &    190 &       31 &      221 &       11 &        2 &       11  \\ 
\cite{Llewellyn06} & 16616 &     16 &       95 &      111 &        2 &        5 &        5  \\ 
& 16617 &     16 &       89 &      105 &        2 &        5 &        5  \\ 
& 16618 &     25 &       75 &      100 &        2 &        5 &        6  \\ 
& 16619 &     13 &       60 &       73 &        2 &        6 &        6  \\ 
\cite{Novitsky09} & 34382 &     37 &        5 &       42 &        3 &        1 &        4  \\ 
& 34391 &     13 &       38 &       51 &        3 &        5 &        6  \\ 
& 34393 &     25 &       74 &       99 &        2 &        6 &        7  \\ 
& 34396 &      23 &       46 &       69 &        2 &        5 &        5 \\ 
& 34397 &      26 &       25 &       51 &        2 &        2 &        4  \\ 
& 34399 &      61 &       88 &      149 &        5 &        9 &       12  \\ 
& 34400 &      54 &       23 &       77 &        4 &        1 &        5  \\ 
& 34405 &      27 &       29 &       56 &        3 &        2 &        4  \\ 
& 34408 &       5 &       73 &       78 &        2 &        8 &        8 \\ 
& 34410 &      35 &       60 &       95 &        2 &        6 &        6 \\ 
& 34411 &      25 &       43 &       68 &        3 &        3 &        6   \\ \hline
\end{tabular}
\end{center}
  \caption{Summary of all the patient data collected from the HIV LANL database -- Patient ID corresponds to the Los Alamos database's Patient ID \citep{LosAlamos}.
   }\label{tab:patients} 
\end{table*}

\subsection{Sequence Aligment} \label{subsec:seqalign}
When applicable, we used the built in MUSCLE 3.8.31 \citep{Muscle04} interface within AliView \citep{AliView14} to align the sequences, which were then visually inspected and cleaned. 
Alignments were trimmed to the interval in which at least  50\% of sequences within the data set had sequence coverage over each codon.
Our simulated sequences contained no insertions or deletions, so no alignment was necessary. 
Additionally, the plasma data set \citep{McCloskey14} had already been aligned and cleaned, so no further alignment was performed.


\subsection{Phylogenetic Reconstruction} \label{subsec:phylo}
In all experiments, our reconstructed phylogenies were maximum likelihood phylogenies reconstructed in FastTree2 \citep{FastTree10} using the GTR model.
We did not attempt to use BEAST \citep{BEAST} to reconstruct any phylogenies from our data.
Leaving dates unspecified on the number of tips would have drastically increased the dimensionality of the problem, and we would therefore need a prohibitively large amount of samples from the MCMC chain to assure convergence to the posterior distribution.

To root these trees, either outgroup rooting against the HIV-1 B ancestor (\href{http://www.hiv.lanl.gov/}{http://www.hiv.lanl.gov/}) reconstruction, or a modified version of RTT regression \citep{APE}. 
Classically, RTT regression considers all tips and chooses the root that minimizes some error metric between the maximum likelihood estimated clock and the tip dates. 
For this application, we simply ignore the tips that are undetermined (either censored or otherwise unknown) in the optimization of the root. 
Our synthetic data was only rooted using this modified RTT scheme, for all real data, both outgroup rooting and RTT regression were used to root the phylogeny. 



\subsection{Date Reconstruction} \label{subsec:daterecon}
Once the trees had been rooted, to reconstruct dates, a general linear model with the normal family was constructed. 
The expected amount of evolution was taken to be the response and the sampling dates of the data were taken as the input. 
These models were only calibrated over the uncensored/plasma data for the simulated and real data sets respectively.
We then use that linear model to predict the dates of the latent/unknown sequences.


\subsection{Hypothesis Testing} \label{subsec:hypot}
To screen for data that are amenable to our analysis, we used the nested log ratio test \citep{Ho14}. 
Our null model was a linear model with zero slope over the expected amount of evolution over time. 
We rejected any sequence data sets whose phylogeny couldn't reject the null model with threshold $\alpha=0.01$.


\subsection{Experiments} \label{subsec:experiments}
For all experiments where it was possible, we used three error metrics to assess  our methodology: the mean square error for predicted date, as well as the mean and median difference between the predicted date and sampling date, where difference defined as simply the predicted date minus the observed date.
To facilitate comparisons among longitudinal data sets, before any analysis was done all tip dates were re-scaled to lie between $0$ and $1$. 
Explicitly, we take $$ \text{new time} = \frac{\text{time} - \text{min time}}{\text{max time} - \text{min time}}$$ which gives {\em normalized time}. Our error metrics are then calculated from this number. This gives normalized mean square error, normalized mean difference, and normalized median difference. 

Our first validation experiment on synthetic data was preformed as such: first we choose 50\% of the tips of a simulated phylogeny and censored those dates, we next used our  modified RTT regression \citep{APE} on the remaining data to calibrate the phylogeny's molecular clock. 
For each censored date we reconstructed the expected date using the expected number of substitutions, then collected the three error metrics above.
For the simulated latent data, we collected the normalized mean square error, normalized mean difference, and normalized median difference, between both the simulated collection dates. We also collect these for the known ``archival'' date.

To evaluate the effectiveness of this methodology on real data, we utilized our {\em plasma} data set \citep{McCloskey14} and performed similar experiments. 
For every applicable patient, we censored 50\% of their known sampling dates then reconstructed the censored dates using a linear model calibrated over the uncensored dates. 
We also performed this experiment with RTT regression rooting and out-group rooting. The goal of this experiment was to parallel the type of results of the first experiment on simulated data. 

Finally, we looked at our {\em mixed} data set comprised of both PBMC and plasma data. 
We used both RTT regression and out-group rooting to root the phylogeny, then calibrated a clock to the known plasma sampling dates and reconstructed the expected age of the sequences for all patients who could reject the null model (Section \ref{subsec:hypot}). 
Here we only collected the normalized errors between the prediction and the collection.


